- Objectif: Comprendre le fonctionnement d'un neurone
- Run:
  - from [Julia](https://julialang.org) we use Pluto:
```Julia
using Pluto
Pluto.run()
```
  - and load the notebook

---

# ğŸ§  Roadmap : des dÃ©rivÃ©es â†’ gradients â†’ neurone rÃ©el

Objectif final : comprendre profondÃ©ment ce qui se passe dans un neurone (type perceptron / MLP), mathÃ©matiquement
et intuitivement.

## Ã‰tape 0 â€” Vision globale

Un neurone =

```
entrÃ©e â†’ somme pondÃ©rÃ©e â†’ fonction dâ€™activation â†’ sortie
```

MathÃ©matiquement :

```
y = Ïƒ(wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b)
```

Lâ€™apprentissage consiste Ã  ajuster les poids `w` pour minimiser une erreur.

Et tout repose sur :
â†’ dÃ©rivÃ©es
â†’ gradients
â†’ descente de gradient

# ğŸ§® Ã‰tape 1 â€” Comprendre la dÃ©rivÃ©e comme "sensibilitÃ©"

## Objectif

Comprendre :

> "Si je change un paramÃ¨tre un tout petit peu, quâ€™est-ce que Ã§a change au rÃ©sultat ?"

Dans Pluto :

* tracer une fonction simple
* afficher sa dÃ©rivÃ©e
* voir lâ€™effet local

Exemples Ã  explorer :

```
f(x) = xÂ²
f(x) = sin(x)
f(x) = e^x
```

ExpÃ©riences :

* zoomer autour dâ€™un point
* visualiser la pente
* approximer numÃ©riquement :

```
f'(x) â‰ˆ (f(x+h) - f(x)) / h
```

Intuition clÃ© :

> Une dÃ©rivÃ©e mesure une influence.

# ğŸ“‰ Ã‰tape 2 â€” Minimisation dâ€™une fonction (descente de gradient 1D)

## Objectif

Comprendre comment on "apprend" sans neurone.

ProblÃ¨me :

```
minimiser f(x) = (x-3)^2
```

Algorithme :

```
x = x - Î· * f'(x)
```

Explorer :

* diffÃ©rents learning rates
* convergence
* divergence

Pluto est parfait pour :

* slider sur Î·
* voir la trajectoire

Concept clÃ© :

> Apprendre = descendre une pente.

# ğŸŒ„ Ã‰tape 3 â€” Gradient en dimension 2+

Passer Ã  :

```
f(x, y) = xÂ² + yÂ²
```

Comprendre :

* gradient = direction de plus forte montÃ©e
* descente = direction opposÃ©e

ExpÃ©riences :

* champ de vecteurs
* trajectoire dâ€™optimisation

Intuition clÃ© :

> Le gradient indique "oÃ¹ corriger".

# ğŸ”— Ã‰tape 4 â€” RÃ¨gle de la chaÃ®ne (LE cÅ“ur du deep learning)

Câ€™est le moment le plus important.

Comprendre :

```
f(x) = sin(xÂ²)
```

DÃ©rivÃ©e :

```
f'(x) = cos(xÂ²) * 2x
```

Puis gÃ©nÃ©raliser :

```
z = f(g(x))
```

La dÃ©rivÃ©e se propage.

Concept clÃ© :

> Lâ€™erreur peut remonter dans un systÃ¨me composÃ©.

Câ€™est exactement :
**backpropagation**

# âš™ï¸ Ã‰tape 5 â€” Construire un neurone Ã  la main

CrÃ©er un neurone simple :

```
y = Ïƒ(wx + b)
```

Choisir :

* Ïƒ = sigmoid
* ou tanh
* ou ReLU

Puis dÃ©finir une erreur :

```
L = (y - target)^2
```

Et calculer :

```
dL/dw
dL/db
```

# ğŸ” Ã‰tape 6 â€” Backpropagation (sans lâ€™appeler comme Ã§a)

Faire un mini systÃ¨me :

```
x â†’ neurone â†’ erreur
```

Puis :

* calculer dÃ©rivÃ©es Ã  la main
* ajuster w
* observer lâ€™apprentissage

Câ€™est LE moment "aha".

# ğŸ§± Ã‰tape 7 â€” Plusieurs entrÃ©es

Passer Ã  :

```
y = Ïƒ(w1x1 + w2x2 + b)
```

Visualiser :

* plan de dÃ©cision
* frontiÃ¨re linÃ©aire

Comprendre :

> Un neurone = sÃ©parateur linÃ©aire.

# ğŸŒ Ã‰tape 8 â€” Plusieurs neurones (MLP)

Construire :

```
2 entrÃ©es â†’ 3 neurones â†’ 1 sortie
```

Et voir apparaÃ®tre :

* non-linÃ©aritÃ©
* formes complexes
* XOR

Moment clÃ© :

> Un seul neurone est limitÃ©.
> Plusieurs = intelligence Ã©mergente.

# ğŸ§ª Ã‰tape 9 â€” Pluto vs Clojure (comment mixer)

### Pluto â†’ idÃ©al pour :

* visualisation
* intuition
* maths
* sliders
* exploration

### Clojure â†’ idÃ©al pour :

* live coding
* systÃ¨mes dynamiques
* exploration structurelle
* manipulations fonctionnelles

# ğŸ§© Suggestion dâ€™architecture hybride

## Julia/Pluto

* explorer maths
* tracer gradients
* visualiser optimisation

## Clojure

* implÃ©menter neurones
* expÃ©rimenter architectures
* jouer avec propagation

# ğŸ§­ Ordre conseillÃ© (trÃ¨s important)

1. dÃ©rivÃ©es 1D
2. gradient descent 1D
3. gradients 2D
4. rÃ¨gle de la chaÃ®ne
5. 1 neurone
6. apprentissage du neurone
7. multi-neurones

# ğŸ’¡ Projet fil rouge (excellent pour apprendre)

CrÃ©er un neurone qui apprend Ã  approximer :

```
y = xÂ²
```

Puis :

```
y = sin(x)
```

Puis :

```
classifier des points 2D
```
