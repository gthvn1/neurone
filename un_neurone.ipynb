{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7167294-03e8-4d3c-9fcd-4e47c3ecd61c",
   "metadata": {},
   "source": [
    "On va commencer par le début: la dérivé\n",
    "\n",
    "- soit une fonction f(x)\n",
    "- Sa dérivée, de manière intuitive, peut s’écrire comme\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x)}{h} \\quad \\text{pour un petit } h\n",
    "$$\n",
    "\n",
    "Elle donne la **pente de la tangente** en ce point. Cela permet de savoir si l’on monte ou descend, et donc si l’on s’approche ou s’éloigne d’un minimum.\n",
    "\n",
    "Comme on le verra un peu plus tard, cette notion va nous permettre de **faire une descente de gradient** sur la fonction d’erreur.\n",
    "\n",
    "L’apprentissage ajuste les poids en suivant le gradient de la fonction d’erreur. Une fonction d’erreur quadratique est pratique car sa dérivée est simple et continue. Les neurones eux peuvent rester linéaires ou avoir des activations non-linéaires, ce qui permet au réseau de modéliser des relations complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0576b-6e51-4940-96cd-c7826f5160b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
